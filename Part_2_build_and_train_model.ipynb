{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os, glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder_decoder.pkl', 'rb') as f:\n",
    "    encode_decode_labels = pickle.load(f)\n",
    "classes = encode_decode_labels['classes']\n",
    "to_class = encode_decode_labels['to_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I've chosen `albumentations` transforms because this library uses opencv under the hood and it is more convenient than original torchvision.transforms which uses PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([Resize(image_size, image_size),\n",
    "                     Normalize(\n",
    "                         mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5]\n",
    "                     )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeseeDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 classes,\n",
    "                 transform=None,\n",
    "                 size=32):\n",
    "        \"\"\"\n",
    "        df: dataframe with all data\n",
    "        classes: dict with map to encode labels\n",
    "        transform: whether to transform data\n",
    "        size: size of train/test image\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        file = self.df.iloc[index]\n",
    "        # get image\n",
    "        image = self.__load_image(file.filename)\n",
    "        \n",
    "        # get label\n",
    "        word = file.label\n",
    "        label = self.classes[word]\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        else:\n",
    "            image = image / 255.\n",
    "        image = torch.from_numpy(image).float().permute([2, 0, 1])\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "    def __load_image(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        # if original image larger we downscale else we upscale\n",
    "        if min(img.shape) > self.size:\n",
    "            img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            img = cv2.resize(img, (self.size, self.size))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset:\n",
    "dataset_train = WeseeDataset(train, classes, transform=transform, size=image_size)\n",
    "# validation dataset:\n",
    "dataset_valid = WeseeDataset(valid, classes, transform=transform, size=image_size)\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4)\n",
    "\n",
    "# do not shuffle for validation\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "['dark', 'ok', 'dark', 'ok']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels =  next(iter(train_loader))\n",
    "print(images[:4].shape)\n",
    "print([to_class[l] for l in labels[:4].numpy()])\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "imshow(torchvision.utils.make_grid(images[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = LeNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "prev_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.073, Val Loss: 1.025, Val Acc: 0.471\n",
      "Model Saved. val acc: 0.471\n",
      "Epoch: 2, Train Loss: 0.927, Val Loss: 0.848, Val Acc: 0.765\n",
      "Model Saved. val acc: 0.765\n",
      "Epoch: 3, Train Loss: 0.719, Val Loss: 0.626, Val Acc: 0.824\n",
      "Model Saved. val acc: 0.824\n",
      "Epoch: 4, Train Loss: 0.507, Val Loss: 0.530, Val Acc: 0.706\n",
      "Epoch: 5, Train Loss: 0.372, Val Loss: 0.519, Val Acc: 0.765\n",
      "Epoch: 6, Train Loss: 0.343, Val Loss: 0.497, Val Acc: 0.824\n",
      "Epoch: 7, Train Loss: 0.306, Val Loss: 0.570, Val Acc: 0.824\n",
      "Epoch: 8, Train Loss: 0.250, Val Loss: 0.461, Val Acc: 0.824\n",
      "Epoch: 9, Train Loss: 0.321, Val Loss: 0.555, Val Acc: 0.824\n",
      "Epoch: 10, Train Loss: 0.258, Val Loss: 0.665, Val Acc: 0.824\n",
      "Epoch: 11, Train Loss: 0.235, Val Loss: 0.449, Val Acc: 0.824\n",
      "Epoch: 12, Train Loss: 0.137, Val Loss: 0.437, Val Acc: 0.824\n",
      "Epoch: 13, Train Loss: 0.173, Val Loss: 0.430, Val Acc: 0.824\n",
      "Epoch: 14, Train Loss: 0.099, Val Loss: 0.568, Val Acc: 0.824\n",
      "Epoch: 15, Train Loss: 0.100, Val Loss: 0.551, Val Acc: 0.824\n",
      "Epoch: 16, Train Loss: 0.105, Val Loss: 0.539, Val Acc: 0.824\n",
      "Epoch: 17, Train Loss: 0.086, Val Loss: 0.611, Val Acc: 0.824\n",
      "Epoch: 18, Train Loss: 0.076, Val Loss: 0.485, Val Acc: 0.824\n",
      "Epoch: 19, Train Loss: 0.129, Val Loss: 0.633, Val Acc: 0.824\n",
      "Epoch: 20, Train Loss: 0.075, Val Loss: 0.532, Val Acc: 0.882\n",
      "Model Saved. val acc: 0.882\n",
      "Epoch: 21, Train Loss: 0.057, Val Loss: 0.537, Val Acc: 0.824\n",
      "Epoch: 22, Train Loss: 0.070, Val Loss: 0.723, Val Acc: 0.824\n",
      "Epoch: 23, Train Loss: 0.109, Val Loss: 0.433, Val Acc: 0.941\n",
      "Model Saved. val acc: 0.941\n",
      "Epoch: 24, Train Loss: 0.085, Val Loss: 0.442, Val Acc: 0.882\n",
      "Epoch: 25, Train Loss: 0.149, Val Loss: 0.594, Val Acc: 0.824\n",
      "Epoch: 26, Train Loss: 0.102, Val Loss: 0.910, Val Acc: 0.824\n",
      "Epoch: 27, Train Loss: 0.083, Val Loss: 0.538, Val Acc: 0.824\n",
      "Epoch: 28, Train Loss: 0.080, Val Loss: 0.483, Val Acc: 0.882\n",
      "Epoch: 29, Train Loss: 0.052, Val Loss: 0.387, Val Acc: 0.941\n",
      "Epoch: 30, Train Loss: 0.037, Val Loss: 0.394, Val Acc: 0.941\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "            \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # training:\n",
    "    model.train()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                       \n",
    "        optimizer.step()\n",
    "\n",
    "    # validation:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(images)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            _, predicted = torch.max(pred, dim=1)\n",
    "            val_loss.append(loss.item())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # calculate acc\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    lr = None\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    print(\"Epoch: {}, Train Loss: {:.3f}, Val Loss: {:.3f}, Val Acc: {:.3f}\".format(\n",
    "        epoch+1, np.mean(train_loss), np.mean(val_loss), val_acc))\n",
    "\n",
    "    # save checkpoint\n",
    "    if val_acc > prev_acc:\n",
    "        prev_acc = val_acc\n",
    "        path = './model/model.pth'\n",
    "        state = {'epoch': epoch + 1,\n",
    "                 'state_dict': model.state_dict(),\n",
    "                 'optimizer': optimizer.state_dict()}\n",
    "        torch.save(state, path)\n",
    "        print(\"Model Saved. val acc: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load model (checkpoint)\n",
    "def load_checkpoint(model, optimizer, device, filename='checkpoint.pth'):\n",
    "    # Note: input model and optimizer should be pre-defined. this routine only updates their states\n",
    "    epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    model.to(device)\n",
    "    # now individually transfer the optimizer parts\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './model/model.pth'\n",
      "=> loaded checkpoint './model/model.pth' (epoch 23)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "filename = './model/model.pth'\n",
    "\n",
    "model, optimizer, prev_acc = load_checkpoint(model,\n",
    "                                             optimizer,\n",
    "                                             device,\n",
    "                                             filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('valid.csv')\n",
    "dataset_valid = WeseeDataset(valid, classes, transform=transform, size=image_size)\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 94.12%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in valid_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    pred = model(images)\n",
    "    _, predicted = torch.max(pred, dim=1)\n",
    "    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "# calculate acc\n",
    "val_acc = correct / total\n",
    "\n",
    "print(\"Validation accuracy: {:.2f}%\".format(val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
